{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thayer-ENGS108/Assignment_2_Fall2022/blob/main/assignment_2_Fall2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd0qJjGWPDEY"
      },
      "source": [
        "# **ENGS 108 Fall 2022 Assignment 2**\n",
        "\n",
        "*Due September 30, 2022 at 11:59PM on Github*\n",
        "\n",
        "**Instructors:** George Cybenko\n",
        "\n",
        "**TAs:** Chase Yakaboski and Clement Nyanhongo\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **Rules and Requirements**\n",
        "\n",
        "\n",
        "1.   You are only allowed to use Python packages that are explicity imported in \n",
        "the assignment notebook or are standard (bultin) python libraries like random, os, sys, etc, (Standard Bultin Python libraries will have a Python.org documentation). For this assignment you may use:\n",
        "  *   [numpy](https://numpy.org/doc/stable/)\n",
        "  *   [pandas](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "  *   [scikit-learn](https://scikit-learn.org/stable/)\n",
        "  *   [matplotlib](https://matplotlib.org/)\n",
        "\n",
        "2.   All code must be fit into the designated code or text blocks in the assignment notebook. They are indentified by a **TODO** qualifier.\n",
        "\n",
        "3. For analytical questions that don't require code, type your answer cleanly in Markdown. For help, see the [Google Colab Markdown Guide](https://colab.research.google.com/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AD5eAz9acxe9"
      },
      "outputs": [],
      "source": [
        "''' Import Statements '''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy import signal\n",
        "import matplotlib.collections as collections\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcqjf9VXHfF"
      },
      "source": [
        "# **Data Loading**\n",
        "Upload the red and synthetic datasets to your google colab session using Google Drive. Read the following [tutorial](https://github.com/Thayer-ENGS108/Assignment_2_Fall2022) for how to get setup. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSQSg2HkXaHF"
      },
      "outputs": [],
      "source": [
        "#TODO: Set your base datasets path. This is my base path, you will need to change to match yours. \n",
        "dataset_base_path = 'datasets/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r2Nya5yV3hW"
      },
      "outputs": [],
      "source": [
        "#-- Everything else you should not need to change.\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "#-- Gather paths\n",
        "synth_data_path = os.path.join(dataset_base_path, 'assign_2_synth_data.pk')\n",
        "red_train_path = os.path.join(dataset_base_path, 'red_train.csv')\n",
        "red_valid_path = os.path.join(dataset_base_path, 'red_valid.csv')\n",
        "red_test_path = os.path.join(dataset_base_path, 'red_test.csv')\n",
        "synth_train_path = os.path.join(dataset_base_path, 'synth_train.csv')\n",
        "synth_valid_path = os.path.join(dataset_base_path, 'synth_valid.csv')\n",
        "synth_test_path = os.path.join(dataset_base_path, 'synth_test.csv')\n",
        "\n",
        "#-- Load Synth_Data\n",
        "with open(synth_data_path, 'rb') as f_:\n",
        "  synth_data = pickle.load(f_)\n",
        "\n",
        "#-- Load Red Wine Data\n",
        "red_train_df = pd.read_csv(red_train_path)\n",
        "red_valid_df = pd.read_csv(red_valid_path)\n",
        "red_test_df = pd.read_csv(red_test_path)\n",
        "synth_train_df = pd.read_csv(synth_train_path)\n",
        "synth_valid_df = pd.read_csv(synth_valid_path)\n",
        "synth_test_df = pd.read_csv(synth_test_path)\n",
        "\n",
        "#-- Data is stored in a tuple of format (X, y) and are already converted to numpy arrays.\n",
        "red_train = (red_train_df.drop('quality', axis=1).to_numpy(), red_train_df['quality'].to_numpy())\n",
        "red_valid = (red_valid_df.drop('quality', axis=1).to_numpy(), red_valid_df['quality'].to_numpy())\n",
        "red_test = (red_test_df.drop('quality', axis=1).to_numpy(), red_test_df['quality'].to_numpy())\n",
        "\n",
        "#-- Load in Synth train, valid, test data with tuple format (X, y)\n",
        "synth_train = (synth_train_df.drop('y', axis=1).to_numpy(), synth_train_df['y'].to_numpy())\n",
        "synth_valid = (synth_valid_df.drop('y', axis=1).to_numpy(), synth_valid_df['y'].to_numpy())\n",
        "synth_test = (synth_test_df.drop('y', axis=1).to_numpy(), synth_test_df['y'].to_numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mty9AB1bd5hH"
      },
      "source": [
        "## **Problem 1: $K$-Means Clustering**\n",
        "In this problem, you will solve a clustering\n",
        "task using the k-means algorithm and an associated classification task using $k$ nearest neighbors algorithm, both of which you learned in class. The dataset for this problem is a synthetic two-dimensional dataset *synth_data*. Each entry has two features $(x_1, x_2)$.\n",
        ">\n",
        "> **Part 1** A reasonable first step in every machine learning task is to understand the dataset at hand. Proceed to explore this problemâ€™s dataset by addressing the following:\n",
        ">> **(a)** Choose a suitable type of plot and visualize the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA0BN4jbfHe5"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here. Use matplotlib for visualization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_Ln5qixfHwt"
      },
      "source": [
        ">> **(b)** From your plot, how many clusters, $k$, would you estimate are represented in the dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAbCaDqZbL1x"
      },
      "source": [
        "**TODO:** *Type your answer in Markdown here.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgn4c2MOg0iJ"
      },
      "source": [
        "> **Part 2** Build a model.\n",
        ">> **(a)** Using the k-Means algorithm, implement a clustering model. *Hint: Use [scikit-learn's K-means library](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTw1K-iDg_MB"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here. Hint: Just define a model, don't train yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzcU0DxOg_nP"
      },
      "source": [
        ">> **(b)** Train the clustering model on several reasonable values of $k$, taking into account your visual inspection from 1b. Plot the sum of distance (SSE) from each data point and its respective cluster for 10 different values of $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBApnd9Fg_05"
      },
      "outputs": [],
      "source": [
        "def train(k, dataset):\n",
        "  ''' Using your model above, implement a function that will train your K-means\n",
        "  for different values of k on your dataset and return the trained model'''\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEOowF_m3bJO"
      },
      "outputs": [],
      "source": [
        "def calculateSSE(model):\n",
        "  ''' Using a trained model calculate the SSE for the model '''\n",
        "\n",
        "  return sse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Dcu3Bp4U2J"
      },
      "outputs": [],
      "source": [
        "#TODO: Choose 10 different values of k based on your inspection and plot the SSE scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udx0GcDrhALm"
      },
      "source": [
        ">> **(c)** What value of $k$ is optimal? How does it compare to your visual inspection?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c905CDZt4_k1"
      },
      "source": [
        "**TODO:** *Type your answer in Markdown here.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pFw9xJn5BNE"
      },
      "outputs": [],
      "source": [
        "#TODO: Write code and plot a graph showing the optimal value of k."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSJ9s6hN5TUu"
      },
      "source": [
        "# **Problem 2: $k$-NN Classification**\n",
        "In this problem, you will utilize data deriving\n",
        "from the same synthetic dataset as above. This time, the data has been separated into *synth_train*, *synth_valid* and *synth_test* arrays. Furthermore, each sample now includes a class label found in the $y$ column. These class labels come from the set $\\{1, 2, . . . , 31\\}$. *Note: These are not the same datasets as Problem 1.* \n",
        "\n",
        "> **Part 1** Train an implementation of the $k$-Nearest Neighbors algorithm on the training dataset. Note that $k$ here refers to the number of neighbors, not clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Lzq0T1555gV"
      },
      "outputs": [],
      "source": [
        "def train(k, dataset):\n",
        "  ''' Implement a function that will train a k-NN\n",
        "  for different values of k on your dataset and return the trained model'''\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaCsGyAq550M"
      },
      "source": [
        "> **Part 2** Report the classification accuracy of this model on the validation set for different values for $k$. Plot these accuracies against $k$ and report the optimal value for $k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sLOw8_u56Lw"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvzjgCff56r4"
      },
      "source": [
        "> **Part 3** Report the classification precision, recall and F1-score of this model on the data in synth test.csv using the optimal value of $k$ that you found in Part 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YQm5vtb56x6"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZVKAG2E64s5"
      },
      "source": [
        "# **Problem 3: Decision Tree Classification**\n",
        "In this problem you will use decision\n",
        "trees to classify the quality of red vinho verde wine samples based on their physicochemical properties. The dataset has been separated into *red_train, red_valid and red_test* arrays. For all of these files, the rightmost column (â€œqualityâ€) is the target label for each datapoint. All other columns are features.\n",
        "\n",
        "> **Part 1** First letâ€™s explore the datasets through the following exercises. Note that we cannot plot the data in a meaningful way given that number of features exceed the physical dimensions.\n",
        "\n",
        ">> **(a)** How many datapoints are in the training, validation, and testing sets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoDNjQON7nxS"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0Yt1Yaf7oEF"
      },
      "source": [
        ">> **(b)** How many features are available for each datapoint?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5xtChuE7oSN"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVyz3oIb7obw"
      },
      "source": [
        ">> **(c)** What are the average *alcohol* and *pH* values for *training* samples?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0OsKVlUH7o_K"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBZ-bK0A8Bsm"
      },
      "source": [
        "> **Part 2** Decision Trees.\n",
        "\n",
        ">> **(a)** Implement a binary decision tree model for the training data. *Hint: Try looking at the [scikit-learn decision tree library](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html?highlight=decision%20tree#sklearn.tree.DecisionTreeClassifier).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wTQK93A8jwe"
      },
      "outputs": [],
      "source": [
        "def train(dataset, max_depth=None):\n",
        "  ''' Implement a function that will train a decision tree model\n",
        "  on your dataset and return the trained model'''\n",
        "  \n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxZJJn3Z86X6"
      },
      "source": [
        ">> **(b)** There are a number of hyperparameters that can be tuned to improve your model, one of which is the criteria for ending the splitting process. Two common ways of terminating the splitting process are *maximum depth* of the tree or *minimum number of samples* left. Tune the *maximum depth* of the tree by reporting the accuracy of the classifier in 2a on the validation set for different settings of *maximum depth*. Plot your findings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jA6sF8X9VPy"
      },
      "outputs": [],
      "source": [
        "#TODO: Write your code here and plot your results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_0VndV69z6H"
      },
      "source": [
        ">> **(c)** Use the optimum setting of *maximum depth* found in 2b to report the accuracy of the classifier on the *test* dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-IBzhZ_9kOv"
      },
      "outputs": [],
      "source": [
        "#TODO: Write"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl-go5CevthF"
      },
      "source": [
        "# **Problem 4: Systems - Estimating ODE Parameters**\n",
        "\n",
        "Many real-world systems can be modelled by linear diffferential equations. Some of the most common examples are mechanical and electrical oscillations (see mass-spring example below) which can be described by the solution of an initial value problem of the form:\n",
        "\n",
        "$$ð‘Žð‘¥â€³+ð‘ð‘¥â€²+ð‘ð‘¥=ð‘”(ð‘¡)  \\tag{1}$$    \n",
        "\n",
        ", where initial condition are given by:       $ð‘¥(0)=ð‘¥0$,   $ð‘¥â€²(0)=ð‘¥â€²0$                    \n",
        " \n",
        "For our problems, we will assume that $g(t) = 0$, no external force (for spring system etc)\n",
        "\n",
        "![Url](https://benmoseley.blog/wp-content/uploads/2021/08/oscillator.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sF_034avthF"
      },
      "source": [
        ">**Part 1** Lets generate some synthetic data using an ODE for a vibration with no damping in chapter 3.7 Example 4 (Source: Elementary Differential Equations and Boundary Value Problems by Boyce & DiPrima, Wiley 2017).\n",
        "In this system, $$ x'' + 0.125x' + x= 0 \\tag{2}$$ \n",
        "and the analytical solution is the function below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGrCSL5svthG"
      },
      "outputs": [],
      "source": [
        "t = np.linspace(0, 30*np.pi, 1000)   # time\n",
        "x_funct = lambda t: (32/np.sqrt(255))*np.exp(-1*t/16)*np.cos((np.sqrt(255)/16)*t -0.06254)  # function to get x given t\n",
        "\n",
        "# analytic function x given t \n",
        "x_analytic = x_funct(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI3JXXP5vthG"
      },
      "source": [
        ">> **(a)** Now lets assume we have observed a noisy sample composed of the first 20% of x_analytic. Create noisy data for the first 20% of x_analytic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqQNpv1rvthG"
      },
      "outputs": [],
      "source": [
        "# create t_noisy (time) to record time for the first 20% of t \n",
        "NOISY_FACTOR = 5 # controls the threshold for adding noise\n",
        "\n",
        "len_t = int(0.2*len(t))\n",
        "t_noisy = t[:len_t]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tUrJ0vYavthG"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute x for the corresponding t_noisy\n",
        "# x = ??"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9xkX0tSvthH"
      },
      "outputs": [],
      "source": [
        "# TODO: adding noise\n",
        "noise = np.array(np.random.random(len_t) - 0.5)/NOISY_FACTOR\n",
        "# x_noisy = ??  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSVb093TvthH"
      },
      "source": [
        ">>**(b)** Our task in this question is to estimate parameters a, b, and c, assuming that we only observed x_noisy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cX6yFodEvthH"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot the observed noisy data below (time vs displacement)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeBj73pevthI"
      },
      "source": [
        ">> **(c)** Real-world data is often noisy and denoising can help to reduce the noise. Denoise the above data to create x_denoised:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zus0e3bJvthI"
      },
      "outputs": [],
      "source": [
        "# denoising\n",
        "N, Wn = 5, 0.03   # Feel free to modify N and Wn as you see fit!\n",
        "b, a = signal.butter(N, Wn, analog=False)   # module from scipy\n",
        "x_denoised = signal.filtfilt(b,a,x_noisy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmfCY55LvthI"
      },
      "outputs": [],
      "source": [
        "# TODO: Plot and insert legend to differentiate x_noisy and x_denoised vs time (on same plot)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0nn9OyJvthI"
      },
      "source": [
        "**Part 2**: Compute derivatives x' and x'' to estimate a, b, and c given x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlosVpFZvthJ"
      },
      "source": [
        ">>**a** Using the ([forward method (finite difference)](https://en.wikipedia.org/wiki/Finite_difference)).\n",
        "compute $x'$ and $x''$ for both x_noisy and x_denoised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdLVrQ1PvthJ"
      },
      "outputs": [],
      "source": [
        "#TODO: Complete the function below\n",
        "def first_derivative(X, dt):\n",
        "    # approximate derivative using forward nethod \n",
        "\n",
        "    return first_derivative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a--kAycJvthJ"
      },
      "outputs": [],
      "source": [
        "#TODO: Complete the functions below\n",
        "def second_derivative(X_first, dt):\n",
        "    # Basically differentiate the first derivative\n",
        "    \n",
        "    return second_derivative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwuz41glvthK"
      },
      "outputs": [],
      "source": [
        "def get_derivatives (X):\n",
        "    dt = t[1] - t[0] # time difference\n",
        "    X_prime =  first_derivative(deepcopy(X), dt)\n",
        "    X_prime_squared = second_derivative(deepcopy(X_prime), dt) \n",
        "    # adjust to make equal lengths arrays\n",
        "    return X[2:], X_prime[1:], X_prime_squared\n",
        "    \n",
        "    \n",
        "# for noisy data \n",
        "x, x_prime, x_prime_squared = get_derivatives(x_noisy)\n",
        "\n",
        "# for denoised data\n",
        "x1, x_prime1, x_prime_squared1 = get_derivatives(x_denoised)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6aeKMKMvthK"
      },
      "outputs": [],
      "source": [
        "# TO DO: Fill the function belwo\n",
        "\n",
        "def plot_figs (x, x_first, x_second):\n",
        "    #TD DO: On same graph, plot x, x', x''\n",
        "\n",
        " #   return;\n",
        "\n",
        "\n",
        "plot_figs(x, x_prime, x_prime_squared)\n",
        "plot_figs(x1, x_prime1, x_prime_squared1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjmyanjOvthK"
      },
      "source": [
        ">>**(b)** How do the derivative plots compare for the noisy vs the denoised samples? Whats the effect of denoising? What happens when we adjust the NOISY_FACTOR (see Part 1a)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckmtqTNHvthK"
      },
      "outputs": [],
      "source": [
        "# TODO: Your answer in Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNT2XUMSvthK"
      },
      "source": [
        ">>**(c)** Now we have x, x' and x''. Since g(t) = 0; we can estimate a, b, and c via regression. If we assume c = 1, \n",
        "then Equation 1 can be written as:\n",
        "    \n",
        "    \n",
        "$$ð‘Žð‘¥â€³+ð‘ð‘¥â€² =  -x \\tag{3}$$\n",
        "\n",
        "\n",
        ">>>From Equation 3, we can perform [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to estimate parameters a and b. Using -x as your dependent variable, and x' and x'' as your independent variables. Train a regression model below:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgYIPs3zvthL"
      },
      "outputs": [],
      "source": [
        "#TODO: Fill the function below\n",
        "\n",
        "def train_model (X, X_first, X_second):\n",
        "    \"\"\" X - original x, X_first - first derivative, X_second - second derivative \"\"\"\n",
        "    \n",
        "    # TODO: Using Equation 3 with independent variable, (X'' and X'), dependent variable (-X).\n",
        "    #       Fit a linear regression model\n",
        "    \n",
        "    # return the regression coefficients and the model (which we will be a and b)\n",
        "    return model.coef_\n",
        "\n",
        "# train regression models for the noisy and denoised data\n",
        "coeff_noisy  = train_model(x, x_prime, x_prime_squared)   # noisy data\n",
        "coeff_denoised = train_model(x1, x_prime1, x_prime_squared1)   # denoised data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lNzZbXsvthL"
      },
      "source": [
        "**Part 3**  From the model coefficients, we can identify parameters $a$ and $b$ and we know that $c = 1$. Now, our task is to \n",
        "predict how good our model can predict the entire dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMj5NVkJvthL"
      },
      "outputs": [],
      "source": [
        "a_noisy, b_noisy = coeff_original \n",
        "a_denoised, b_denoised = coeff_denoised \n",
        "\n",
        "print('For the noisy sample: (a = {}, b = {}, c = 1)'.format(a_noisy, b_noisy))\n",
        "print('For the denoised sample: (a = {}, b = {}, c = 1)'.format(a_denoised, b_denoised))\n",
        "print(\"The analytic solution has (a = 1, b = 0.125, and c = 1)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJL5s5ZgvthL"
      },
      "source": [
        ">>**(a)** How do estimated parameters from the noisy and denoised samples compare to the analytic parameters? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxU5zG89vthM"
      },
      "outputs": [],
      "source": [
        "# TODO: Your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sI0y-STdvthM"
      },
      "source": [
        ">> **(b)** From Equation 3, $$x = -1*(ax'' + bx') \\tag{4}$$ \n",
        "We will use this equation to test how good our parameters predict the analytic solution (given x' and x'')."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhx3bfO6vthM"
      },
      "outputs": [],
      "source": [
        "# get derivatives on entire dataset \n",
        "X, X_prime, X_prime_squared = get_derivatives(x_analytic)\n",
        "\n",
        "\n",
        "# TODO: Use Equation 3 to compute X\n",
        "def compute_x (X_first, X_second, a, b):\n",
        "    \n",
        "    # Fill here\n",
        "    return X\n",
        "\n",
        "# TODO: Predict Y for the noisy sample, and the denoised sample \n",
        "x_pred_noisy = compute_x(X_prime, X_prime_squared, a_noisy, b_noisy)\n",
        "x_pred_denoised  = compute_x(X_prime, X_prime_squared, a_denoised, b_denoised)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3TQlrLSvthM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# adjust t to fit dimensions of predictions \n",
        "LEN_T = len(t)\n",
        "t_original = t[:LEN_T-2]\n",
        "x_original = x_analytic[:LEN_T -2]\n",
        "\n",
        "\n",
        "# Plots to show how well our parameters fit the data from the analytic solution\n",
        "fig, axes = plt.subplots(1, 2, figsize = (15, 5))\n",
        "axes[0].plot(t_original, x_original, '*', color = 'green', label = 'analytic')\n",
        "axes[0].plot(t_original, x_pred_noisy, '*', color = 'red', label = 'noisy prediction')\n",
        "\n",
        "axes[1].plot(t_original, x_pred_denoised, '*', color = 'blue', label = 'denoised prediction')\n",
        "axes[1].plot(t_original, x_original, '*', color = 'green', label = 'analytic')\n",
        "\n",
        "axes[0].legend()\n",
        "axes[1].legend()\n",
        "\n",
        "# This shades the seen part (in creating the model -yellow), but the model\n",
        "# extends to the unseen white part\n",
        "axes[0].axvspan(0, t[len_t], color='y', alpha=0.5, lw=0)\n",
        "axes[1].axvspan(0, t[len_t], color='y', alpha=0.5, lw=0)\n",
        "\n",
        "axes[0].set_xlabel('time')\n",
        "axes[0].set_ylabel('displacement')\n",
        "axes[1].set_ylabel('time')\n",
        "axes[1].set_ylabel('displacement')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHyss5qQvthN"
      },
      "source": [
        ">>**(c)** How do these two plots compare? On the same graph, plot the relative errors to compare how x_pred noisy and x_pred_denoised differ from x_original (if necessary, use a logarithmic scale). $$ RE(true, pred) =|true -pred|/|true|$$ What conclusion do you get from the relative errors?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBw8sFpKvthN"
      },
      "outputs": [],
      "source": [
        "# Plots here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0XFTPJVvthO"
      },
      "source": [
        ">>**(d)** In real-world scenarios, we often dont have x_analytic but only the noisy sample. How would you ensure if \n",
        "the parameters you obtained are good enough if you only have x_noisy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJyGFeaBvthO"
      },
      "outputs": [],
      "source": [
        "# Answer here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
